{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### README\n",
    "This notebook is to be ran whenever there is an update to the module mappings on myedurec\n",
    "\n",
    "Some manual work will need to be done to check if any new universities have been added, and we will have to update\n",
    "the country/continent those universities are in, as well as cross-reference with existing universities to see if they\n",
    "are the same entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, requests\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import copy\n",
    "import pycountry_convert as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fname is the csv file downloaded from myedurec\n",
    "fname = \"myedurec_retrieved_160721.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize df\n",
    "# This encoding is required because there are some non-ASCII characters\n",
    "df = pd.read_csv(fname, encoding=\"iso-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_name(string):\n",
    "    if type(string) != str:\n",
    "        return string\n",
    "    else:\n",
    "        return string.replace('???',\"'\")\n",
    "\n",
    "def clean_data(original_data):\n",
    "    data = original_data\n",
    "    not_nan = lambda x: x is not np.nan\n",
    "    data = data[data['Partner University'].map(not_nan)]\n",
    "    data = data[data['PU Module 1 Title'].map(not_nan) | data['PU Module 2 Title'].map(not_nan)]\n",
    "    data = data[data['NUS Module 1 Title'].map(not_nan) | data['NUS Module 2 Title'].map(not_nan)]\n",
    "    data = data[data['NUS Module 1'].map(not_nan) | data['NUS Module 2'].map(not_nan)]\n",
    "    data['PU Module 1 Title'] = data['PU Module 1 Title'].map(proper_name)\n",
    "    data['PU Module 2 Title'] = data['PU Module 2 Title'].map(proper_name)\n",
    "    data['NUS Module 1 Title'] = data['NUS Module 1 Title'].map(proper_name)\n",
    "    data['NUS Module 2 Title'] = data['NUS Module 2 Title'].map(proper_name)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = clean_data(df)\n",
    "cleaned_data.to_csv(\"cleaned_mappings.csv\",index=False)\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ THIS BEFORE RUNNING THE FOLLOWING CHUNK OF CODE!\n",
    "Rename the old 'school_country_continent_mapping.csv' to 'OLD.csv' before running!\n",
    "\n",
    "This block of code has some work done manually since there is no easy way to check\n",
    "which country each school is in. We will have to add the countries in manually if \n",
    "we miss out the first step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('OLD.csv')\n",
    "\n",
    "# countries = set(df1['Country'])\n",
    "# full_continent_name = {\n",
    "#     'AF': 'Africa',\n",
    "#     'AS': 'Asia',\n",
    "#     'EU': 'Europe',\n",
    "#     'NA': 'North America',\n",
    "#     'OC': 'Oceania',\n",
    "#     'SA': 'South America'\n",
    "# }\n",
    "\n",
    "# for i in range(len(df1)):\n",
    "#     if df1['Country'][i] != 'UNKNOWN':\n",
    "#         c = pc.country_name_to_country_alpha2(df1['Country'][i], cn_name_format=\"default\")\n",
    "#         continent = pc.country_alpha2_to_continent_code(c)\n",
    "#         df1['Continent'][i] = full_continent_name[continent]\n",
    "#     else:\n",
    "#         df1['Continent'][i] = 'Europe'\n",
    "\n",
    "# df1.to_csv('school_country_continent_mapping.csv',index=False)\n",
    "# del df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "This list is created manually. I could not think of any fast/efficient way to determine if 2 schools\n",
    "are the same entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = pd.read_csv(\"cleaned_mappings.csv\")\n",
    "\n",
    "modified_mappings = mappings.copy()\n",
    "\n",
    "equivalent_schools_mapping = {\n",
    "    'Aalto University': ['Aalto University, Helsinki'],\n",
    "    'Aarhus School of Business': ['Aarhus University'],\n",
    "    'Cornell University': ['Cornell Univ Coll of Agriculture & Life Sciences', 'Cornell Univ Coll of Human Ecology'],\n",
    "    'Georgetown University': ['Georgetown University Law Center','Georgetown University, Washington D.C.'],\n",
    "    'Humboldt University of Berlin': ['Humboldt-Universitaet zu Berlin'],\n",
    "    'Imperial College London': ['Imperial College Business School'],\n",
    "    'Leiden University': ['Leiden University Medical Center (LUMC)'],\n",
    "    'University College London': ['University College London, University of London'],\n",
    "    'Universite Catholique De Louvain': ['Universite Catholique de Louvain'],\n",
    "}\n",
    "\n",
    "for school in equivalent_schools_mapping:\n",
    "    for equiv in equivalent_schools_mapping[school]:\n",
    "        tmp_df = modified_mappings[modified_mappings['Partner University'] == equiv]\n",
    "        modified_mappings['Partner University'].iloc[tmp_df.index] = [school] * len(tmp_df)\n",
    "\n",
    "UCs_df = modified_mappings[modified_mappings['Partner University'] == 'University of California']\n",
    "for school in modified_mappings['Partner University'].unique():\n",
    "    if 'University of California' in school and school != 'University of California':\n",
    "        to_append = UCs_df.copy()\n",
    "        to_append['Partner University'] = [school] * len(to_append)\n",
    "        modified_mappings = modified_mappings.append(to_append)\n",
    "modified_mappings.index = range(len(modified_mappings))\n",
    "\n",
    "school_country_continent_mapping = pd.read_csv('school_country_continent_mapping.csv')\n",
    "country, continent = [], []\n",
    "for i in range(len(modified_mappings)):\n",
    "    school = modified_mappings['Partner University'][i]\n",
    "    df = school_country_continent_mapping[school_country_continent_mapping['Partner University'] == school]\n",
    "    try:\n",
    "        country.append(df['Country'].iloc[0])\n",
    "        continent.append(df['Continent'].iloc[0])\n",
    "    except:\n",
    "        print(school)\n",
    "modified_mappings['Country'] = country\n",
    "modified_mappings['Continent'] = continent\n",
    "\n",
    "modified_mappings.to_csv('cleaned_mappings_with_locations.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following cell is to download the data from the NUSMods api\n",
    "\n",
    "Only run this to update current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import httplib2\n",
    "# from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "# http = httplib2.Http()\n",
    "# url = \"https://nusmods.com/api/v2/2020-2021/\"\n",
    "# status, response = http.request(url + \"modules/\")\n",
    "\n",
    "# links = []\n",
    "# for link in BeautifulSoup(response, parse_only=SoupStrainer('a')):\n",
    "#     if link.has_attr('href') and link['href'][-5:] == '.json':\n",
    "#         links.append(link['href'])\n",
    "        \n",
    "\n",
    "# start = time.time()\n",
    "# for i, link in enumerate(links):\n",
    "#     if (i % 100 == 0):\n",
    "#         print(str(i+1) + '/' + str(len(links)))\n",
    "#     moduleDetails = requests.get(url + f\"modules/{link}\").json()\n",
    "#     with open(\"module_details/\"+f\"{link}\", \"w\") as f:\n",
    "#         json.dump(moduleDetails, f)\n",
    "    \n",
    "# end = time.time()\n",
    "# print(\"Time taken: \" + str(round(end-start)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    arr = []\n",
    "    for fname in sorted(glob.glob('module_details/*.json')):\n",
    "        with open(fname,'r') as f:\n",
    "            data = json.load(f)\n",
    "            arr.append(data)\n",
    "    return arr\n",
    "jsons = load_data()\n",
    "len(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(jsons):\n",
    "    attrs = set()\n",
    "    for data in jsons:\n",
    "        for attribute in data:\n",
    "            attrs.add(attribute)\n",
    "    return attrs\n",
    "attributes = get_attributes(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_preclusions(jsons):\n",
    "    preclusions_mapping = {}\n",
    "    for data in jsons:\n",
    "        if 'preclusion' in data:\n",
    "            preclusions_mapping[data['moduleCode']] = data['preclusion']\n",
    "        else:\n",
    "            preclusions_mapping[data['moduleCode']] = ''\n",
    "    def preclusions_string_to_set(preclusion_string):\n",
    "        # Get all alphaNumeric substrings\n",
    "        alpha_numerics = re.split('[^a-zA-Z0-9]', preclusion_string)\n",
    "\n",
    "        def is_module_code(string):\n",
    "            right_length = len(string) >= 4\n",
    "            all_caps = string == string.upper()\n",
    "            contains_digits = bool(re.search(r'\\d', string))\n",
    "            return right_length and all_caps and contains_digits\n",
    "\n",
    "        return set(filter(is_module_code, alpha_numerics))\n",
    "    for preclusion in preclusions_mapping:\n",
    "        preclusions_mapping[preclusion] = preclusions_string_to_set(preclusions_mapping[preclusion])\n",
    "    to_delete = []\n",
    "    for preclusion in preclusions_mapping:\n",
    "            preclusions_mapping[preclusion] = list(preclusions_mapping[preclusion])\n",
    "    return preclusions_mapping\n",
    "preclusions_mapping = join_preclusions(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preclusionMappings.json', 'w') as f:\n",
    "    json.dump(preclusions_mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equivalent_modules(jsons):\n",
    "    equivalent_modules = join_preclusions(jsons)\n",
    "    \n",
    "    module_titles = {}\n",
    "    for json in jsons:\n",
    "        title = json['title'].upper()\n",
    "        if title not in module_titles:\n",
    "            module_titles[title] = []\n",
    "        module_titles[title].append(json['moduleCode'])\n",
    "\n",
    "    equivalent_modules = copy.deepcopy(preclusions_mapping)\n",
    "    for module_title in module_titles:\n",
    "        equivalents = module_titles[module_title]\n",
    "        for module_code in equivalents:\n",
    "            for equiv in equivalents:\n",
    "                if equiv not in equivalent_modules[module_code]:\n",
    "                    equivalent_modules[module_code].append(equiv)\n",
    "    return equivalent_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equivalent_modules = get_equivalent_modules(jsons)\n",
    "with open('equivalentModuleMappings.json', 'w') as f:\n",
    "    json.dump(equivalent_modules, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = []\n",
    "for j in jsons:\n",
    "    modules.append(f\"{j['moduleCode']} {j['title']}\")\n",
    "modules.sort()\n",
    "\n",
    "with open('../src/data/modules.json', 'w') as f:\n",
    "    json.dump(modules,f)\n",
    "\n",
    "countries = sorted(modified_mappings['Country'].unique())\n",
    "with open('../src/data/countries.json', 'w') as f:\n",
    "    json.dump(countries, f)\n",
    "continents = sorted(modified_mappings['Continent'].unique())\n",
    "with open('../src/data/continents.json', 'w') as f:\n",
    "    json.dump(continents, f)\n",
    "schools = sorted(modified_mappings['Partner University'].unique())\n",
    "with open('../src/data/schools.json', 'w') as f:\n",
    "    json.dump(schools, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s0 = set(modified_mappings['NUS Module 1'].dropna().unique())\n",
    "s1 = set(modified_mappings['NUS Module 2'].dropna().unique())\n",
    "mapped_modules = s0.union(s1)\n",
    "reduced_module_list = set()\n",
    "for module in list(mapped_modules):\n",
    "    if module in equivalent_modules:\n",
    "        reduced_module_list.add(module)\n",
    "        reduced_module_list.update(equivalent_modules[module])\n",
    "        \n",
    "# There is a module code \"2002\" in this list, which must be a bug, hence we removed it\n",
    "reduced_module_list = sorted(list(reduced_module_list))[1:]\n",
    "\n",
    "reduced_modules = []\n",
    "for mod in reduced_module_list:\n",
    "    try:\n",
    "        with open(f\"module_details/{mod}.json\", 'r') as f:\n",
    "                data = json.load(f)\n",
    "                reduced_modules.append(f\"{data['moduleCode']} {data['title']}\")\n",
    "    except:\n",
    "        print(mod)\n",
    "reduced_modules.sort()\n",
    "\n",
    "with open('../src/data/reducedModules.json', 'w') as f:\n",
    "    json.dump(reduced_modules,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "There are modules that can be mapped but are missing from the NUSMods API data. Almost all of these \n",
    "modules are dummy codes, so we will ignore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = set(modified_mappings['NUS Module 1'].dropna().unique())\n",
    "s1 = set(modified_mappings['NUS Module 2'].dropna().unique())\n",
    "s = s0.union(s1)\n",
    "S = set()\n",
    "not_inside = []\n",
    "for mod in s:\n",
    "    try:\n",
    "        S.add(mod)\n",
    "        S.update(set(equivalent_modules[mod]))\n",
    "    except:\n",
    "        not_inside.append(mod)\n",
    "not_inside.sort()\n",
    "\n",
    "vals = [(mod0 in not_inside or mod1 in not_inside) for mod0, mod1 in zip(modified_mappings['NUS Module 1'], modified_mappings['NUS Module 2'])]\n",
    "modified_mappings[vals]['NUS Module 1 Title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
